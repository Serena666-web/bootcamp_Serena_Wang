{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5543dcb2-80ec-47d8-b539-958380b967af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/wangyuhan/bootcamp_Serena_Wang/project\n",
      "Using .env: /Users/wangyuhan/bootcamp_Serena_Wang/project/.env | Exists: True | Has ALPHA key? True\n",
      "DATA_RAW: /Users/wangyuhan/bootcamp_Serena_Wang/project/data/raw\n",
      "Moved from notebooks/data/raw -> project/data/raw: ['api_source-yfinance_symbol-nvda_2025-08-17T19-49-40.csv']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, shutil\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "CWD = Path.cwd()\n",
    "PROJECT_ROOT = CWD.parent if CWD.name == \"notebooks\" else CWD\n",
    "\n",
    "\n",
    "nb_env = CWD / \".env\"\n",
    "root_env = PROJECT_ROOT / \".env\"\n",
    "if CWD.name == \"notebooks\" and nb_env.exists():\n",
    "    # only move if root doesn't already have one\n",
    "    if not root_env.exists():\n",
    "        shutil.move(str(nb_env), str(root_env))\n",
    "        print(f\"Moved .env -> {root_env}\")\n",
    "    else:\n",
    "        print(f\".env already exists at {root_env}; leaving notebooks/.env alone\")\n",
    "\n",
    "load_dotenv(root_env, override=True)\n",
    "ALPHA_KEY = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "nb_raw = CWD / \"data\" / \"raw\"\n",
    "moved = []\n",
    "if CWD.name == \"notebooks\" and nb_raw.exists():\n",
    "    for p in nb_raw.glob(\"*.csv\"):\n",
    "        target = DATA_RAW / p.name\n",
    "        shutil.move(str(p), str(target))\n",
    "        moved.append(target.name)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"Using .env:\", root_env, \"| Exists:\", root_env.exists(), \"| Has ALPHA key?\", bool(ALPHA_KEY))\n",
    "print(\"DATA_RAW:\", DATA_RAW)\n",
    "print(\"Moved from notebooks/data/raw -> project/data/raw:\", moved if moved else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43d4f4ce-9d22-4211-a4fb-a208f2c7adbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Vantage unavailable → Thank you for using Alpha Vantage! This is a premium endpoint. You may subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly unlock all premium endpoints\n",
      "Falling back to yfinance…\n",
      "Shape: (125, 2)\n",
      "NA counts: {'date': 0, 'adj_close': 0}\n",
      "Saved: /Users/wangyuhan/bootcamp_Serena_Wang/project/data/raw/api_source-yfinance_symbol-nvda_2025-08-17T20-00-42.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd, requests\n",
    "\n",
    "try:\n",
    "    validate_df\n",
    "except NameError:\n",
    "    def validate_df(df: pd.DataFrame, *, required_cols, dtypes_map=None, min_rows=10, allow_na_frac=None):\n",
    "        missing = [c for c in required_cols if c not in df.columns]\n",
    "        if missing: raise ValueError(f\"Missing required columns: {missing}\")\n",
    "        if dtypes_map:\n",
    "            for col, kind in dtypes_map.items():\n",
    "                if col in df.columns:\n",
    "                    if str(kind).startswith(\"datetime\"):\n",
    "                        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "                    elif str(kind).startswith(\"float\"):\n",
    "                        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        if len(df) < min_rows:\n",
    "            raise ValueError(f\"Too few rows: {len(df)} < {min_rows}\")\n",
    "        return [f\"Shape: {df.shape}\", f\"NA counts: {df[required_cols].isna().sum().to_dict()}\"]\n",
    "\n",
    "try:\n",
    "    safe_filename\n",
    "except NameError:\n",
    "    def safe_filename(prefix: str, meta: dict, ext=\"csv\"):\n",
    "        stamp = pd.Timestamp.utcnow().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "        parts = [prefix] + [f\"{k}-{str(v).lower()}\" for k, v in meta.items()]\n",
    "        return \"_\".join(parts) + f\"_{stamp}.{ext}\"\n",
    "\n",
    "assert 'ALPHA_KEY' in globals() and 'DATA_RAW' in globals(), \"Run the setup cell first to set ALPHA_KEY and DATA_RAW.\"\n",
    "\n",
    "SYMBOL = \"NVDA\" \n",
    "\n",
    "def fetch_alpha_vantage_adj(symbol: str, api_key: str):\n",
    "    url = \"https://www.alphavantage.co/query\"\n",
    "    params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY_ADJUSTED\",\n",
    "        \"symbol\": symbol,\n",
    "        \"outputsize\": \"compact\",\n",
    "        \"apikey\": api_key,\n",
    "        \"datatype\": \"json\",\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    # throttle/errors → return None and the reason\n",
    "    err = js.get(\"Error Message\") or js.get(\"Information\") or js.get(\"Note\")\n",
    "    if err:\n",
    "        return None, err\n",
    "    key = next((k for k in js.keys() if \"Time Series\" in k), None)\n",
    "    if not key:\n",
    "        return None, f\"Unexpected keys: {list(js.keys())}\"\n",
    "    series = js[key]\n",
    "    df = (\n",
    "        pd.DataFrame(series).T\n",
    "        .rename_axis(\"date\")\n",
    "        .reset_index()\n",
    "        .loc[:, [\"date\", \"5. adjusted close\"]]\n",
    "        .rename(columns={\"5. adjusted close\": \"adj_close\"})\n",
    "    )\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"adj_close\"] = pd.to_numeric(df[\"adj_close\"], errors=\"coerce\")\n",
    "    return df, None\n",
    "\n",
    "def fetch_yf(symbol: str):\n",
    "    import yfinance as yf\n",
    "    ydf = yf.download(symbol, period=\"6mo\", interval=\"1d\",\n",
    "                      auto_adjust=False, group_by=\"column\", progress=False)\n",
    "    if ydf.empty:\n",
    "        raise RuntimeError(f\"yfinance returned empty data for {symbol!r}\")\n",
    "    ydf = ydf.reset_index()\n",
    "    # flatten if MultiIndex\n",
    "    if isinstance(ydf.columns, pd.MultiIndex):\n",
    "        ydf.columns = [\" \".join([str(x) for x in tup if x]).strip() for tup in ydf.columns.to_list()]\n",
    "    cols = [str(c) for c in ydf.columns]\n",
    "    date_col = \"Date\" if \"Date\" in cols else (\"Datetime\" if \"Datetime\" in cols else None)\n",
    "    if not date_col:\n",
    "        raise KeyError(f\"No Date/Datetime column in: {cols}\")\n",
    "    price_candidates = [c for c in cols if c.replace(\" \", \"\").lower().startswith(\"adjclose\")]\n",
    "    if not price_candidates:\n",
    "        price_candidates = [c for c in cols if c.replace(\" \", \"\").lower().startswith(\"close\")]\n",
    "    if not price_candidates:\n",
    "        raise KeyError(f\"Neither 'Adj Close*' nor 'Close*' in: {cols}\")\n",
    "    price_col = price_candidates[0]\n",
    "    df = ydf[[date_col, price_col]].rename(columns={date_col: \"date\", price_col: \"adj_close\"})\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"adj_close\"] = pd.to_numeric(df[\"adj_close\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "source = None\n",
    "if ALPHA_KEY:\n",
    "    df_api, err = fetch_alpha_vantage_adj(SYMBOL, ALPHA_KEY)\n",
    "    if df_api is None:\n",
    "        print(\"Alpha Vantage unavailable →\", err)\n",
    "        print(\"Falling back to yfinance…\")\n",
    "        df_api = fetch_yf(SYMBOL)\n",
    "        source = \"yfinance\"\n",
    "    else:\n",
    "        source = \"alpha\"\n",
    "else:\n",
    "    df_api = fetch_yf(SYMBOL)\n",
    "    source = \"yfinance\"\n",
    "\n",
    "\n",
    "df_api = df_api.sort_values(\"date\").reset_index(drop=True)\n",
    "msgs = validate_df(df_api, required_cols=[\"date\", \"adj_close\"],\n",
    "                   dtypes_map={\"date\": \"datetime64[ns]\", \"adj_close\": \"float\"})\n",
    "print(\"\\n\".join(msgs))\n",
    "\n",
    "fname = safe_filename(\"api\", {\"source\": source, \"symbol\": SYMBOL})\n",
    "out_path = DATA_RAW / fname\n",
    "df_api.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e389a6c9-00aa-4ce2-911a-bfa4f5306197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found wikitable candidates (index, rows, caption snippet):\n",
      "  0: rows=  2 | Sales by business unit (2023)\n",
      "  1: rows=  4 | Sales by region (2023)\n",
      "  2: rows= 10 | 10-year financials (2016–2025)\n",
      "\n",
      "Using table index 2 | rows=10 | caption: 10-year financials (2016–2025)\n",
      "Shape: (10, 4)\n",
      "NA counts: {'Year': 0, 'Revenue (mn. US$)': 0}\n",
      "Saved: /Users/wangyuhan/bootcamp_Serena_Wang/project/data/raw/scrape_source-wikipedia_page-nvidia_table-10-year-financials-2016-2025_2025-08-17T23-08-14.csv\n",
      "\n",
      "Preview:\n",
      "   Year  Revenue (mn. US$)  Net income (mn. US$)  Employees\n",
      "0  2016               5010                   614       9227\n",
      "1  2017               6910                  1666      10299\n",
      "2  2018               9714                  3047      11528\n",
      "3  2019              11716                  4141      13277\n",
      "4  2020              10918                  2796      13775\n",
      "5  2021              16675                  4332      18975\n",
      "6  2022              26914                  9752      22473\n",
      "7  2023              26974                  4368      26000\n",
      "8  2024              60922                 29760      29600\n",
      "9  2025             130497                 72880      36000\n"
     ]
    }
   ],
   "source": [
    "# Scrape Table\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    DATA_RAW\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "    DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "    DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    validate_df\n",
    "except NameError:\n",
    "    def validate_df(df: pd.DataFrame, *, required_cols, dtypes_map=None, min_rows=5, allow_na_frac=None):\n",
    "        missing = [c for c in required_cols if c not in df.columns]\n",
    "        if missing: raise ValueError(f\"Missing required columns: {missing}\")\n",
    "        if dtypes_map:\n",
    "            for col, kind in dtypes_map.items():\n",
    "                if col in df.columns:\n",
    "                    if str(kind).startswith(\"datetime\"):\n",
    "                        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "                    elif str(kind).startswith(\"float\"):\n",
    "                        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        if len(df) < min_rows:\n",
    "            raise ValueError(f\"Too few rows: {len(df)} < {min_rows}\")\n",
    "        return [f\"Shape: {df.shape}\", f\"NA counts: {df[required_cols].isna().sum().to_dict()}\"]\n",
    "\n",
    "try:\n",
    "    safe_filename\n",
    "except NameError:\n",
    "    def safe_filename(prefix: str, meta: dict, ext=\"csv\"):\n",
    "        stamp = pd.Timestamp.utcnow().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "        parts = [prefix] + [f\"{k}-{str(v).lower()}\" for k, v in meta.items()]\n",
    "        return \"_\".join(parts) + f\"_{stamp}.{ext}\"\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = re.sub(r\"\\s*\\[[^\\]]*\\]\\s*\", \" \", s)  # remove [citation] brackets\n",
    "    s = s.replace(\"\\u00a0\", \" \").replace(\"\\u2009\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def parse_wikitable(tbl):\n",
    "    \"\"\"Parse a single wikitable into a DataFrame and return (df, caption_text).\"\"\"\n",
    "    caption = tbl.find(\"caption\")\n",
    "    cap_text = clean_text(caption.get_text(\" \", strip=True)) if caption else \"\"\n",
    "\n",
    "    # pick the first row that actually has THs as header\n",
    "    header_row = None\n",
    "    for tr in tbl.find_all(\"tr\"):\n",
    "        if tr.find(\"th\"):\n",
    "            header_row = tr\n",
    "            break\n",
    "    if header_row is None:\n",
    "        return None\n",
    "\n",
    "    headers = [clean_text(th.get_text(\" \", strip=True)) for th in header_row.find_all([\"th\",\"td\"])]\n",
    "    rows = []\n",
    "    for tr in header_row.find_next_siblings(\"tr\"):\n",
    "        cells = [clean_text(td.get_text(\" \", strip=True)) for td in tr.find_all([\"td\",\"th\"])]\n",
    "        if not cells:\n",
    "            continue\n",
    "        # normalize length to header\n",
    "        if len(cells) < len(headers):\n",
    "            cells += [\"\"] * (len(headers) - len(cells))\n",
    "        elif len(cells) > len(headers):\n",
    "            cells = cells[:len(headers)]\n",
    "        rows.append(cells)\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "    return df, cap_text\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Nvidia\"\n",
    "headers = {\"User-Agent\": \"AFE-Course-Notebook/1.0 (contact: instructor@example.edu)\"}\n",
    "\n",
    "\n",
    "resp = requests.get(URL, headers=headers, timeout=30)\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "tables = soup.select(\"table.wikitable\")\n",
    "candidates = []\n",
    "for tbl in tables:\n",
    "    parsed = parse_wikitable(tbl)\n",
    "    if parsed:\n",
    "        df, cap = parsed\n",
    "        candidates.append((df, cap, len(df)))\n",
    "\n",
    "if not candidates:\n",
    "    raise RuntimeError(\"No parseable wikitables found on the page.\")\n",
    "\n",
    "print(\"Found wikitable candidates (index, rows, caption snippet):\")\n",
    "for i, (df, cap, n) in enumerate(candidates):\n",
    "    print(f\"  {i}: rows={n:>3} | {cap[:60]}\")\n",
    "\n",
    "\n",
    "preferred = [i for i,(df,cap,n) in enumerate(candidates) if \"acquisition\" in cap.lower()]\n",
    "if preferred:\n",
    "    idx = preferred[0]\n",
    "else:\n",
    "    big = [i for i,(df,cap,n) in enumerate(candidates) if n >= 5]\n",
    "    idx = max(big, key=lambda i: candidates[i][2]) if big else max(range(len(candidates)), key=lambda i: candidates[i][2])\n",
    "\n",
    "df_scrape, caption, nrows = candidates[idx]\n",
    "print(f\"\\nUsing table index {idx} | rows={nrows} | caption: {caption or '(no caption)'}\")\n",
    "\n",
    "\n",
    "for c in df_scrape.columns:\n",
    "    s = df_scrape[c].astype(str).str.replace(\",\", \"\", regex=False).str.replace(\"\\u2009\",\"\",regex=False)\n",
    "    # try to detect numerics; keep as text if too few convert\n",
    "    nums = pd.to_numeric(s.str.replace(r\"[^0-9\\.\\-]\", \"\", regex=True), errors=\"coerce\")\n",
    "    if nums.notna().mean() >= 0.5:\n",
    "        df_scrape[c] = nums\n",
    "\n",
    "\n",
    "required = list(df_scrape.columns[:2]) if df_scrape.shape[1] >= 2 else list(df_scrape.columns)\n",
    "msgs = validate_df(df_scrape, required_cols=required, min_rows=5)\n",
    "print(\"\\n\".join(msgs))\n",
    "\n",
    "\n",
    "slug = re.sub(r\"[^a-z0-9]+\", \"-\", (caption.lower() or \"wikitable\")).strip(\"-\")\n",
    "fname = safe_filename(\"scrape\", {\"source\":\"wikipedia\",\"page\":\"nvidia\",\"table\": slug})\n",
    "out_path = DATA_RAW / fname\n",
    "df_scrape.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "print(df_scrape.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8fd170-7394-4b9e-b1de-0eee3452ab81",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "**API Source**\n",
    "- Alpha Vantage `TIME_SERIES_DAILY_ADJUSTED` (fallback: yfinance)\n",
    "- URL: https://www.alphavantage.co/query\n",
    "- Params: `function=TIME_SERIES_DAILY_ADJUSTED`, `symbol=${SYMBOL}`, `outputsize=compact`, `datatype=json`, `apikey=$ALPHAVANTAGE_API_KEY`\n",
    "\n",
    "**Scrape Source**\n",
    "- Wikipedia — Nvidia: https://en.wikipedia.org/wiki/Nvidia\n",
    "- Parsed the first substantial `table.wikitable` (prefer “Acquisitions” if present).\n",
    "\n",
    "**Validation**\n",
    "- API: required cols `date, adj_close`; coerced `date→datetime`, `adj_close→float`; ≥10 rows; NA counts printed.\n",
    "- Scrape: ≥2 cols, ≥5 rows; light numeric coercion; NA counts printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c748a-d19f-416e-bd5e-acf81765982c",
   "metadata": {},
   "outputs": [],
   "source": [
    " **Assumptions & Risks**\n",
    "- Alpha Vantage throttling/premium → fallback to yfinance.\n",
    "- Wikipedia tables may change; parsing uses simple heuristics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
